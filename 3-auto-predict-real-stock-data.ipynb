{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay! This is the last chapter. When we finish this we finish the whole project and we are able to automatically help us with investing stocks WOW!\n",
    "\n",
    "So this time let's make it straight forward. Note that this notebook is for better understanding, we finally need to separate our code in some python files. So let's first write the import module `module.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module.py\n",
    "import quandl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's recall the data processing methods in `preprocess.py`.\n",
    "\n",
    "There are several things need to notice, we do not need to split the data into train and validation now. Instead, we just use all the newest data as training data and use them to train the model. Then, we could make a next-90-days prediction, based on the last 60 days of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess.py\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def get_train_and_test_data(df):    \n",
    "    df['Date'] = pd.to_datetime(df.index,format='%Y-%m-%d')\n",
    "    data = df.sort_index(ascending=True, axis=0)\n",
    "    new_data = pd.DataFrame(index=range(0,len(df)),columns=['Date', 'Close'])\n",
    "    for i in range(0,len(data)):\n",
    "        new_data['Date'][i] = data['Date'][i]\n",
    "        new_data['Close'][i] = data['Close'][i]\n",
    "\n",
    "    #setting index\n",
    "    new_data.index = new_data.Date\n",
    "    new_data.drop('Date', axis=1, inplace=True)\n",
    "\n",
    "    #creating train and test sets\n",
    "    dataset = new_data.values\n",
    "\n",
    "    train = dataset\n",
    "    #valid = dataset[960:,:]  \n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaled_data = scaler.fit_transform(dataset)\n",
    "    \n",
    "    x_train, y_train = [], []\n",
    "    for i in range(60,len(train)):\n",
    "        x_train.append(scaled_data[i-60:i,0])\n",
    "        y_train.append(scaled_data[i,0])\n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "    # reshape to feed into LSTM\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))\n",
    "    \n",
    "    inputs = new_data[len(new_data) - 90 - 60:].values\n",
    "    inputs = inputs.reshape(-1,1)    \n",
    "    inputs = scaler.transform(inputs)    \n",
    "    \n",
    "    x_test = []\n",
    "    for i in range(60, inputs.shape[0]):\n",
    "        x_test.append(inputs[i-60:i,0])\n",
    "    x_test = np.array(x_test)\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1))\n",
    "    # x_test should be \n",
    "    return x_train, y_train, x_test, train[-1], scaler\n",
    "\n",
    "\n",
    "def test_and_plot(model, test, td, vd, scaler, length=90):\n",
    "    res = np.zeros((length,1))\n",
    "    cp = np.copy(test)\n",
    "    for i in range(cp.shape[0]):\n",
    "        res[i][0] = model.predict(cp[i].reshape(1, -1, 1))[0][0]\n",
    "        if i < cp.shape[0] - 1:\n",
    "            cp[i+1][-1][0] = res[i][0]\n",
    "            # print (X_2[i+1])    \n",
    "\n",
    "    #closing_price = model.predict(X_test)\n",
    "    res = scaler.inverse_transform(res)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
